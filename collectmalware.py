#!/usr/bin/env python3
"""
Executable Scanner and Downloader (Corrected & Refined)

Scans websites from recentmalware.txt for executable files with progress tracking.
This version includes recursive scanning, batch processing, file signature
verification, and inspection of ZIP file contents to accurately identify executables.

Corrections:
- Fixed all indentation and spacing errors.
- Resolved race condition by ensuring all access to shared data is thread-safe.
- Corrected the logic for reading streamed ZIP file content.
- Simplified the HTTP/HTTPS pre-flight check.
- Made the number of workers a user-configurable setting.
- MODIFIED: Now checks all files based on signature, regardless of extension.
- REFINED: Removed unused methods for cleaner code.
- FINAL CORRECTION: Removed all file extension checking. The identification is now based purely on file signatures.
- UPDATE: All ZIP files are now downloaded without inspection of their contents.
- NEW: If a downloaded file lacks an extension, one is added based on its detected signature.
- NEW: Added functionality to scan a random sample of domains from the list.
"""

import requests
import os
import re
import random
from urllib.parse import urljoin, urlparse
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import threading
from collections import deque

class ExecutableScanner:
    def __init__(self, subdomain_file="recentmalware.txt", download_dir="downloads"):
        self.subdomain_file = subdomain_file
        self.download_dir = download_dir
        
        # Signatures for common executable file types
        self.MAGIC_NUMBERS = {
            b'MZ': 'PE (Windows EXE/DLL)',
            b'\x7fELF': 'ELF (Linux Executable)',
            b'PK\x03\x04': 'ZIP (APK, JAR, etc.)',
            b'\xca\xfe\xba\xbe': 'Mach-O Universal (macOS)',
            b'\xce\xfa\xed\xfe': 'Mach-O 32-bit (macOS)',
            b'\xcf\xfa\xed\fe': 'Mach-O 64-bit (macOS)',
            b'#!': 'Script (Shell, Python, etc.)'
        }
        
        # Map signature types to file extensions for saving
        self.SIGNATURE_TO_EXTENSION = {
            'PE (Windows EXE/DLL)': '.exe',
            'ELF (Linux Executable)': '.elf',
            'ZIP (APK, JAR, etc.)': '.zip',
            'Mach-O Universal (macOS)': '.macho',
            'Mach-O 32-bit (macOS)': '.macho',
            'Mach-O 64-bit (macOS)': '.macho',
            'Script (Shell, Python, etc.)': '.sh'
        }
        
        # Use a set for found executables to automatically handle duplicates
        self.found_executables = set()
        self.visited_urls = set() # Global set to avoid re-checking any URL across all domains
        self.downloaded_count = 0
        self.scanned_pages = 0
        self.failed_scans = 0
        
        # Add detailed error counters for better diagnostics
        self.preflight_failure_count = 0
        
        # A single lock for all shared data modification to ensure thread safety
        self.lock = threading.Lock()
        self.realtime_download = False

        # Create download directory
        os.makedirs(self.download_dir, exist_ok=True)

        # Setup session with optimized settings for massive scale
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        # Aggressive connection pooling
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=200,
            pool_maxsize=200,
            max_retries=0  # No retries for speed
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def load_subdomains(self):
        """Load subdomains from file"""
        try:
            with open(self.subdomain_file, 'r', errors='ignore') as f:
                subdomains = [line.strip() for line in f if line.strip()]
            print(f"Loaded {len(subdomains)} subdomains from {self.subdomain_file}")
            return subdomains
        except FileNotFoundError:
            print(f"Error: {self.subdomain_file} not found!")
            return []

    def verify_signature(self, data):
        """Checks if the initial bytes of a file match a known executable signature."""
        for magic, name in self.MAGIC_NUMBERS.items():
            if data.startswith(magic):
                return name # Return the type of executable found
        return None

    def extract_links(self, html_content, base_url):
        """Extract all links from HTML content using regex for speed"""
        try:
            # A single comprehensive regex is more efficient
            pattern = r'(?:href|src|action)=["\']([^"\']+)["\']|url\(["\']?([^"\')\s]+)["\']?\)'
            links = set()
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for group in matches:
                match = next((s for s in group if s), None) # Find the non-empty match in the tuple
                if match and not match.startswith(('javascript:', 'mailto:', 'tel:', '#')):
                    full_url = urljoin(base_url, match.strip())
                    links.add(full_url)
            return list(links)
        except Exception:
            return []

    def scan_site_recursive(self, start_url, max_depth=4):
        """
        Recursively scans a website. It crawls pages and inspects file links
        by downloading the first few bytes to check for executable signatures.
        """
        found_on_site = set()
        q = deque([(start_url, 0)])
        domain = urlparse(start_url).netloc
        
        # This set is local to the scan of one site to avoid re-crawling pages
        crawled_pages = {start_url}

        while q:
            current_url, depth = q.popleft()
            
            try:
                page_response = self.session.get(current_url, timeout=10)
                page_response.raise_for_status()
                with self.lock:
                    self.scanned_pages += 1
                
                html_content = page_response.text
                links = self.extract_links(html_content, current_url)

                for link in links:
                    parsed_link = urlparse(link)
                    if parsed_link.netloc != domain:
                        continue # Stay on the same domain

                    path = parsed_link.path.lower()
                    # This logic only identifies pages for crawling. Everything else is treated as a file.
                    is_page = not path or any(path.endswith(ext) for ext in ['/', '.html', '.htm', '.php', '.asp', '.aspx'])
                    
                    if is_page:
                        if depth + 1 < max_depth and link not in crawled_pages:
                            crawled_pages.add(link)
                            q.append((link, depth + 1))
                    else: # If it's not a page, it's a file candidate to be checked by signature
                        # --- THREAD-SAFE BLOCK ---
                        with self.lock:
                            if link in self.visited_urls:
                                continue
                            self.visited_urls.add(link)
                        # --- END THREAD-SAFE BLOCK ---

                        try:
                            with self.session.get(link, stream=True, timeout=15) as r:
                                r.raise_for_status()
                                # Read only the first 1KB to check the signature
                                first_chunk = r.raw.read(1024)
                                signature_type = self.verify_signature(first_chunk)

                                if signature_type:
                                    # A threat is confirmed if any signature is found.
                                    # ZIP files are now downloaded directly without inspection.
                                    with self.lock:
                                        if link not in self.found_executables:
                                            self.found_executables.add(link)
                                            found_on_site.add(link)
                                    
                                    if self.realtime_download:
                                        # Pass the signature type to generate a better filename
                                        filename = self.generate_filename(link, signature_type)
                                        filepath = os.path.join(self.download_dir, filename)
                                        try:
                                            with open(filepath, 'wb') as f:
                                                # Write the first chunk we already read
                                                f.write(first_chunk)
                                                # Stream the rest of the content to the file
                                                for chunk in r.iter_content(chunk_size=8192):
                                                    f.write(chunk)
                                            with self.lock:
                                                self.downloaded_count += 1
                                        except Exception:
                                            pass # Ignore download write errors
                        except (requests.RequestException, IOError):
                            continue # Ignore errors fetching individual files
            except requests.RequestException:
                continue # Ignore errors fetching pages
        
        return list(found_on_site)

    def process_subdomain(self, subdomain, max_depth):
        """
        Manages the scanning of a single subdomain, from pre-flight check to recursive scan.
        """
        urls_to_try = [f"https://{subdomain}", f"http://{subdomain}"]
        scan_url = None

        for url in urls_to_try:
            try:
                # Use a HEAD request for a quick pre-flight check
                self.session.head(url, timeout=5, allow_redirects=True)
                scan_url = url
                break # Success, proceed with this URL
            except requests.RequestException:
                continue # Try the next URL
        
        if not scan_url:
            with self.lock:
                self.preflight_failure_count += 1
                self.failed_scans += 1
            return []

        return self.scan_site_recursive(scan_url, max_depth)

    def generate_filename(self, url, signature_type=None):
        """
        Generate safe filename from URL.
        Appends an extension based on signature if the URL path doesn't have one.
        """
        try:
            parsed = urlparse(url)
            # Sanitize domain and path to create a unique-ish filename
            domain = parsed.netloc.replace('.', '_')
            path = parsed.path.strip('/').replace('/', '_')
            filename = f"{domain}_{path}"
            if not path:
                filename = f"{domain}_index"
        except Exception:
            # Fallback for weird URLs
            filename = f"download_{hash(url) % 1000000}"
        
        # Remove invalid characters
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)

        # Append extension based on signature if the filename doesn't have one
        if signature_type:
            # A simple check to see if there's already a file extension
            _, ext = os.path.splitext(filename)
            if not ext or len(ext) > 5: # If no extension or it's very long (likely not an extension)
                extension = self.SIGNATURE_TO_EXTENSION.get(signature_type)
                if extension:
                    filename += extension
        
        # Limit total length
        return filename[:200]

    def run_scan(self, subdomains_to_scan, max_workers=200, download_files=False, max_depth=4, batch_size=10000):
        """Run the complete scan process with batching."""
        subdomains = subdomains_to_scan
        if not subdomains:
            return

        print(f"\nStarting scan of {len(subdomains):,} subdomains...")
        print(f"Using {max_workers} concurrent workers.")
        print(f"Recursive scan depth set to {max_depth}.")
        print(f"Processing in batches of {batch_size:,}.")
        
        self.realtime_download = download_files
        if download_files:
            print(f"✓ Real-time downloading enabled to '{self.download_dir}' directory.")
        
        total_batches = (len(subdomains) + batch_size - 1) // batch_size
        start_time = time.time()
        
        try:
            total_domains_processed = 0
            for batch_num in range(total_batches):
                batch_start = batch_num * batch_size
                batch_end = min((batch_num + 1) * batch_size, len(subdomains))
                batch_subdomains = subdomains[batch_start:batch_end]
                
                print(f"\n--- Processing Batch {batch_num + 1}/{total_batches} ({len(batch_subdomains):,} domains) ---")
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    with tqdm(total=len(batch_subdomains), desc=f"Batch {batch_num + 1}",
                              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
                        
                        future_to_subdomain = {
                            executor.submit(self.process_subdomain, subdomain, max_depth): subdomain
                            for subdomain in batch_subdomains
                        }
                        
                        for future in as_completed(future_to_subdomain):
                            subdomain = future_to_subdomain[future]
                            try:
                                found_files = future.result()
                                if found_files:
                                    pbar.write(f"✓ Found {len(found_files)} verified executables on {subdomain}")
                            except Exception as exc:
                                pbar.write(f"✗ Error processing {subdomain}: {exc}")
                            finally:
                                pbar.update(1)
                
                total_domains_processed += len(batch_subdomains)
                elapsed = time.time() - start_time
                rate = total_domains_processed / elapsed if elapsed > 0 else 0
                print(f"Batch {batch_num + 1} summary: Total Processed: {total_domains_processed:,}, "
                      f"Failed: {self.failed_scans:,}, Pages Scanned: {self.scanned_pages:,}, Found: {len(self.found_executables):,}, "
                      f"Downloaded: {self.downloaded_count:,}, Rate: {rate:.1f}/sec")

        finally:
            print("\nScan loop finished or interrupted.")

        elapsed_time = time.time() - start_time
        scan_rate = len(subdomains) / elapsed_time if elapsed_time > 0 else 0
        final_found_list = sorted(list(self.found_executables))
        
        print(f"\n{'='*70}")
        print(f"SCAN COMPLETE")
        print(f"{'='*70}")
        print(f"Total domains processed: {len(subdomains):,}")
        print(f"Total pages scanned: {self.scanned_pages:,}")
        print(f"Total failed initial scans: {self.failed_scans:,}")
        
        if len(subdomains) > 0:
            successful_scans = len(subdomains) - self.failed_scans
            success_rate = (successful_scans / len(subdomains) * 100)
            print(f"Domain success rate: {success_rate:.1f}%")
        
        print("\n--- Failure Analysis (Initial Connection) ---")
        print(f"Skipped (Pre-flight Fail): {self.preflight_failure_count:,}")
        
        print(f"\nUnique VERIFIED executables found: {len(final_found_list):,}")
        print(f"Files downloaded: {self.downloaded_count:,}")
        print(f"Time elapsed: {elapsed_time:.2f} seconds ({elapsed_time/3600:.2f} hours)")
        print(f"Average domain scan rate: {scan_rate:.2f} domains/second")
        
        if final_found_list:
            print(f"\n--- Found Executables (First 50) ---")
            for i, exe_url in enumerate(final_found_list[:50], 1):
                print(f"{i:4d}. {exe_url}")
            
            if len(final_found_list) > 50:
                print(f"... and {len(final_found_list) - 50:,} more")
        
        return final_found_list

def get_user_input(prompt, default):
    """Generic function to get validated positive integer input from the user."""
    while True:
        try:
            choice = input(prompt).strip()
            if not choice:
                return default
            value = int(choice)
            if value > 0:
                return value
            else:
                print("Please enter a positive number.")
        except ValueError:
            print("Invalid input. Please enter a number.")

def main():
    """Main function"""
    print("Executable Scanner and Downloader (Deep Scan + Signature Verification)")
    print("=" * 75)
    
    scanner = ExecutableScanner()
    subdomains = scanner.load_subdomains()
    if not subdomains:
        print("No subdomains to scan. Exiting.")
        return
    
    # --- Logic to ask for random sampling (default: yes) ---
    sample_choice = input("\nDo you want to scan a random sample of these domains? (y/n, default is y): ").lower().strip()
    if sample_choice == '' or sample_choice == 'y':
        default_sample = len(subdomains) // 2
        prompt = f"Enter the number of domains to randomly select (default is {default_sample:,}): "
        sample_size = get_user_input(prompt, default_sample)
        if sample_size >= len(subdomains):
            print(f"Warning: Sample size ({sample_size:,}) is not smaller than total domains. Scanning all {len(subdomains):,} domains.")
        else:
            print(f"Selecting a random sample of {sample_size:,} subdomains to scan.")
            subdomains = random.sample(subdomains, sample_size)
    
    download_choice = input("\nDo you want to download found executables? (y/n): ").lower().strip()
    download_files = download_choice == 'y'
    
    if download_files:
        print(f"Files will be downloaded to: {os.path.abspath(scanner.download_dir)}")

    max_depth = get_user_input("Enter scan depth (e.g., 4 for homepage and its links, default is 4): ", 4)
    batch_size = get_user_input("Enter batch size (number of domains per batch, default is 10000): ", 10000)
    max_workers = get_user_input("Enter number of concurrent workers (e.g., 200, default is 200): ", 200)

    found_executables = scanner.run_scan(
        subdomains_to_scan=subdomains,
        max_workers=max_workers, 
        download_files=download_files, 
        max_depth=max_depth, 
        batch_size=batch_size
    )
    
    if found_executables:
        results_file = "found_executables.txt"
        with open(results_file, 'w', errors='ignore') as f:
            for exe_url in found_executables:
                f.write(f"{exe_url}\n")
        print(f"\nResults saved to: {os.path.abspath(results_file)}")

if __name__ == "__main__":
    main()
